{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from Tensorflow Object Detection Framework\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
    "Tensorflow Object Detection Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import track.bagtracker as bagtrack\n",
    "import track.tracker as track\n",
    "import scripts.detector as od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to clasterize and merge Lines detected by cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoughBundler:\n",
    "    '''Clasterize and merge each cluster of cv2.HoughLinesP() output\n",
    "    a = HoughBundler()\n",
    "    foo = a.process_lines(houghP_lines, binary_image)\n",
    "    '''\n",
    "\n",
    "    def get_orientation(self, line):\n",
    "        '''get orientation of a line, using its length\n",
    "        https://en.wikipedia.org/wiki/Atan2\n",
    "        '''\n",
    "        orientation = math.atan2(abs((line[0] - line[2])), abs((line[1] - line[3])))\n",
    "        return math.degrees(orientation)\n",
    "\n",
    "    def checker(self, line_new, groups, min_distance_to_merge, min_angle_to_merge):\n",
    "        '''Check if line have enough distance and angle to be count as similar\n",
    "        '''\n",
    "        for group in groups:\n",
    "            # walk through existing line groups\n",
    "            for line_old in group:\n",
    "                # check distance\n",
    "                if self.get_distance(line_old, line_new) < min_distance_to_merge:\n",
    "                    # check the angle between lines\n",
    "                    orientation_new = self.get_orientation(line_new)\n",
    "                    orientation_old = self.get_orientation(line_old)\n",
    "                    # if all is ok -- line is similar to others in group\n",
    "                    if abs(orientation_new - orientation_old) < min_angle_to_merge:\n",
    "                        group.append(line_new)\n",
    "                        return False\n",
    "        # if it is totally different line\n",
    "        return True\n",
    "\n",
    "    def DistancePointLine(self, point, line):\n",
    "        \"\"\"Get distance between point and line\n",
    "        http://local.wasp.uwa.edu.au/~pbourke/geometry/pointline/source.vba\n",
    "        \"\"\"\n",
    "        px, py = point\n",
    "        x1, y1, x2, y2 = line\n",
    "\n",
    "        def lineMagnitude(x1, y1, x2, y2):\n",
    "            'Get line (aka vector) length'\n",
    "            lineMagnitude = math.sqrt(math.pow((x2 - x1), 2) + math.pow((y2 - y1), 2))\n",
    "            return lineMagnitude\n",
    "\n",
    "        LineMag = lineMagnitude(x1, y1, x2, y2)\n",
    "        if LineMag < 0.00000001:\n",
    "            DistancePointLine = 9999\n",
    "            return DistancePointLine\n",
    "\n",
    "        u1 = (((px - x1) * (x2 - x1)) + ((py - y1) * (y2 - y1)))\n",
    "        u = u1 / (LineMag * LineMag)\n",
    "\n",
    "        if (u < 0.00001) or (u > 1):\n",
    "            # // closest point does not fall within the line segment, take the shorter distance\n",
    "            # // to an endpoint\n",
    "            ix = lineMagnitude(px, py, x1, y1)\n",
    "            iy = lineMagnitude(px, py, x2, y2)\n",
    "            if ix > iy:\n",
    "                DistancePointLine = iy\n",
    "            else:\n",
    "                DistancePointLine = ix\n",
    "        else:\n",
    "            # Intersecting point is on the line, use the formula\n",
    "            ix = x1 + u * (x2 - x1)\n",
    "            iy = y1 + u * (y2 - y1)\n",
    "            DistancePointLine = lineMagnitude(px, py, ix, iy)\n",
    "\n",
    "        return DistancePointLine\n",
    "\n",
    "    def get_distance(self, a_line, b_line):\n",
    "        \"\"\"Get all possible distances between each dot of two lines and second line\n",
    "        return the shortest\n",
    "        \"\"\"\n",
    "        dist1 = self.DistancePointLine(a_line[:2], b_line)\n",
    "        dist2 = self.DistancePointLine(a_line[2:], b_line)\n",
    "        dist3 = self.DistancePointLine(b_line[:2], a_line)\n",
    "        dist4 = self.DistancePointLine(b_line[2:], a_line)\n",
    "\n",
    "        return min(dist1, dist2, dist3, dist4)\n",
    "\n",
    "    def merge_lines_pipeline_2(self, lines):\n",
    "        'Clusterize (group) lines'\n",
    "        groups = []  # all lines groups are here\n",
    "        # Parameters to play with\n",
    "        min_distance_to_merge = 50\n",
    "        min_angle_to_merge = 20\n",
    "        # first line will create new group every time\n",
    "        groups.append([lines[0]])\n",
    "        # if line is different from existing gropus, create a new group\n",
    "        for line_new in lines[1:]:\n",
    "            if self.checker(line_new, groups, min_distance_to_merge, min_angle_to_merge):\n",
    "                groups.append([line_new])\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def merge_lines_segments1(self, lines):\n",
    "        \"\"\"Sort lines cluster and return first and last coordinates\n",
    "        \"\"\"\n",
    "        orientation = self.get_orientation(lines[0])\n",
    "\n",
    "        # special case\n",
    "        if (len(lines) == 1):\n",
    "            return [lines[0][:2], lines[0][2:]]\n",
    "\n",
    "        # [[1,2,3,4],[]] to [[1,2],[3,4],[],[]]\n",
    "        points = []\n",
    "        for line in lines:\n",
    "            points.append(line[:2])\n",
    "            points.append(line[2:])\n",
    "        # if vertical\n",
    "        if 45 < orientation < 135:\n",
    "            # sort by y\n",
    "            points = sorted(points, key=lambda point: point[1])\n",
    "        else:\n",
    "            # sort by x\n",
    "            points = sorted(points, key=lambda point: point[0])\n",
    "\n",
    "        # return first and last point in sorted group\n",
    "        # [[x,y],[x,y]]\n",
    "        return [points[0], points[-1]]\n",
    "\n",
    "    def process_lines(self, lines):\n",
    "        '''Main function for lines from cv.HoughLinesP() output merging\n",
    "        for OpenCV 3\n",
    "        lines -- cv.HoughLinesP() output\n",
    "        img -- binary image\n",
    "        '''\n",
    "        lines_x = []\n",
    "        lines_y = []\n",
    "        # for every line of cv2.HoughLinesP()\n",
    "        for line_i in [l[0] for l in lines]:\n",
    "            orientation = self.get_orientation(line_i)\n",
    "            # if vertical\n",
    "            if 45 < orientation < 135:\n",
    "                lines_y.append(line_i)\n",
    "            else:\n",
    "                lines_x.append(line_i)\n",
    "\n",
    "        lines_y = sorted(lines_y, key=lambda line: line[1])\n",
    "        lines_x = sorted(lines_x, key=lambda line: line[0])\n",
    "        merged_lines_all = []\n",
    "\n",
    "        # for each cluster in vertical and horizantal lines leave only one line\n",
    "        for i in [lines_x, lines_y]:\n",
    "            if len(i) > 0:\n",
    "                groups = self.merge_lines_pipeline_2(i)\n",
    "                merged_lines = []\n",
    "                for group in groups:\n",
    "                    merged_lines.append(self.merge_lines_segments1(group))\n",
    "\n",
    "                merged_lines_all.extend(merged_lines)\n",
    "\n",
    "        return merged_lines_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to detect lane line using Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lanes(imgDisplay):\n",
    "    # convert to grayscale then black/white to binary image\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = 200\n",
    "    frame = cv2.threshold(frame, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # cv2.imshow(\"Black/White\", frame)\n",
    "\n",
    "    # blur image to help with edge detection\n",
    "    blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "    # cv2.imshow(\"Blurred\", blurred)\n",
    "\n",
    "    # identify edges & show on screen\n",
    "    edged = cv2.Canny(blurred, 30, 150)\n",
    "    # cv2.imshow(\"Edged\", edged)\n",
    "\n",
    "    # perform full Hough Transform to identify lane lines\n",
    "    # lines = cv2.HoughLines(edged, 1, np.pi / 180, 25)\n",
    "    unmerged_lines = cv2.HoughLinesP(\n",
    "        edged,\n",
    "        rho=6,\n",
    "        theta=np.pi / 60,\n",
    "        threshold=150,\n",
    "        lines=np.array([]),\n",
    "        minLineLength=60,\n",
    "        maxLineGap=5\n",
    "    )\n",
    "    global emergency_lane_lines\n",
    "    if unmerged_lines is None:\n",
    "        emergency_lane_lines = []\n",
    "        return\n",
    "    merger = HoughBundler()\n",
    "    lines = merger.process_lines(unmerged_lines)\n",
    "    # print(lines)\n",
    "    for line in lines:\n",
    "        line[0][0] += x\n",
    "        line[0][1] += y\n",
    "        line[1][0] += x\n",
    "        line[1][1] += y\n",
    "        cv2.line(imgDisplay, (line[0][0], line[0][1]), (line[1][0], line[1][1]), (0, 0, 255), 3)\n",
    "    # define arrays for left and right lanes\n",
    "    emergency_lane_lines = lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to draw out tracked vehicle in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTrackedVehicle(imgDisplay):\n",
    "    car = 0\n",
    "    truck = 0\n",
    "    motor = 0\n",
    "    bus = 0\n",
    "    for fid in cartracker.faceTrackers.keys():\n",
    "        tracked_position = cartracker.faceTrackers[fid].get_position()\n",
    "        t_x = int(tracked_position.left())\n",
    "        t_y = int(tracked_position.top())\n",
    "        t_w = int(tracked_position.width())\n",
    "        t_h = int(tracked_position.height())\n",
    "        t_x_bar = t_x + 0.5 * t_w\n",
    "        t_y_bar = t_y + 0.5 * t_h\n",
    "        min_dist = [float('inf'), ]\n",
    "        for line in emergency_lane_lines:\n",
    "            p3 = np.array([t_x_bar, t_y_bar])\n",
    "            p1 = np.array([line[0][0], line[0][1]])\n",
    "            p2 = np.array([line[1][0], line[1][1]])\n",
    "            d = np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)\n",
    "            min_dist.append(abs(d))\n",
    "        StoppedTime = cartracker.getStoppedTime(fid)\n",
    "        direction = cartracker.direction[fid]\n",
    "        type = cartracker.type[fid]\n",
    "        if type == 'Car':\n",
    "            car += 1\n",
    "            rectColor = (0, 255, 0)\n",
    "        elif type == 'Truck':\n",
    "            truck += 1\n",
    "            rectColor = (0, 255, 255)\n",
    "        elif type == 'Motorcycle':\n",
    "            motor += 1\n",
    "            rectColor = (255, 255, 0)\n",
    "        else:\n",
    "            bus += 1\n",
    "            rectColor = (255, 0, 0)\n",
    "        # if StoppedTime>5:\n",
    "        #     rectColor = (0,0,255)\n",
    "        #     text = '{}{} Stopped'.format(type,fid) + str(int(StoppedTime)) + 's'\n",
    "        #\n",
    "        # else:\n",
    "        text = '{}{} '.format(type, fid) + str(direction)\n",
    "        if min(min_dist) < 60:\n",
    "            rectColor = (0, 0, 255)\n",
    "            text = '{}{} '.format(type, fid) + 'Emergency Lane Driving'\n",
    "            print('found emergency lane driving {}{}'.format(type, fid))\n",
    "        textSize = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        textX = int(t_x + t_w / 2 - (textSize[0]) / 2)\n",
    "        textY = int(t_y)\n",
    "        textLoc = (textX, textY)\n",
    "\n",
    "        cv2.rectangle(imgDisplay, (t_x, t_y),\n",
    "                      (t_x + t_w, t_y + t_h),\n",
    "                      rectColor, 1)\n",
    "\n",
    "        cv2.putText(imgDisplay, text, textLoc,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255, 255, 255), 1)\n",
    "    return car, motor, bus, truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arg(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Script for emergency lane driving detection')\n",
    "    parser.add_argument('-m', '--model',\n",
    "                        help='Tensorflow object detection model path',\n",
    "                        required=True,\n",
    "                        default='model/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb')\n",
    "    parser.add_argument('-i', '--input',\n",
    "                        help='Input video filename',\n",
    "                        required=True)\n",
    "    parser.add_argument('-o', '--output',\n",
    "                        help='Filename for output video',\n",
    "                        default='output.avi')\n",
    "    parser.add_argument('-f', '--frame_interval',\n",
    "                        help='Amount of frame interval between frame processing',\n",
    "                        default=5)\n",
    "    parser.add_argument('-vt', '--vehicle_threshold',\n",
    "                        help='Threshold value for vehicle detection',\n",
    "                        default=0.8)\n",
    "\n",
    "    results = parser.parse_args(args)\n",
    "    return (results.model,\n",
    "            results.input,\n",
    "            results.output,\n",
    "            results.frame_interval,\n",
    "            results.vehicle_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_arg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ed1a4cdec1e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# parsing arguments as variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvehiclethres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mframe_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_interval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvehiclethres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvehiclethres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_arg' is not defined"
     ]
    }
   ],
   "source": [
    "# parsing arguments as variable\n",
    "model_path, input, output, frame_interval, vehiclethres = check_arg(sys.argv[1:])\n",
    "frame_interval = int(frame_interval)\n",
    "vehiclethres = float(vehiclethres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables manually\n",
    "model_path = 'model/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'\n",
    "input = 'videos/A0051.mov'\n",
    "output = 'output/sample_output.avi'\n",
    "frame_interval = 5\n",
    "vehiclethres = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing \n",
    "id = 0\n",
    "carid = 0\n",
    "cartracker = cartrack.Tracker()\n",
    "emergency_lane_lines = []\n",
    "odapi = od.DetectorAPI(path_to_ckpt=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating cv2 capture and get video information\n",
    "cap = cv2.VideoCapture(input)\n",
    "flag, frame = cap.read()\n",
    "assert flag == True\n",
    "height, width, _ = frame.shape\n",
    "# defining frame size for tracker to detect movement out of video frame\n",
    "cartracker.videoFrameSize = frame.shape\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cartracker.fps = fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video writer creation\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "frame_count = 0\n",
    "# Define VideoWrite object\n",
    "# cv2.VideoWrite('arg1',arg2,arg3,(width,heigh))\n",
    "# arg1:output file name\n",
    "# arg2:Specify Fourcc code\n",
    "# arg3: frames per seconds\n",
    "# FourCC is a 4-byte code used to specify video codec\n",
    "out = cv2.VideoWriter(output, fourcc, fps, (width, height))\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 18000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to process video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    r, img = cap.read()\n",
    "    # if there are still frame to process.\n",
    "    if r:\n",
    "        \n",
    "        if frame_count % (frame_interval * 3) == 0:\n",
    "            # remove duplicate car tracker\n",
    "            cartracker.removeDuplicate()\n",
    "        # update and\n",
    "        cartracker.deleteTrack(img)\n",
    "        if frame_count % frame_interval == 0:\n",
    "            boxes, scores, classes, num = odapi.processFrame(img)\n",
    "            # Visualization of the results of a detection.\n",
    "            for i in range(len(boxes)):\n",
    "                # Class 3 represents car\n",
    "                if classes[i] == 3 and scores[i] > vehiclethres:\n",
    "                    box = boxes[i]\n",
    "                    matchedID = cartracker.getMatchId(img, (box[1], box[0], box[3], box[2]))\n",
    "                    if matchedID is None:\n",
    "                        carid += 1\n",
    "                        cartracker.createTrack(img, (box[1], box[0], box[3], box[2]), str(carid), scores[i], 'Car')\n",
    "                # Class 4 represents motorcycle\n",
    "                elif classes[i] == 4 and scores[i] > vehiclethres:\n",
    "                    box = boxes[i]\n",
    "                    matchedID = cartracker.getMatchId(img, (box[1], box[0], box[3], box[2]))\n",
    "                    if matchedID is None:\n",
    "                        carid += 1\n",
    "                        cartracker.createTrack(img, (box[1], box[0], box[3], box[2]), str(carid), scores[i],\n",
    "                                               'Motorcycle')\n",
    "                # Class 6 represents Bus\n",
    "                elif classes[i] == 6 and scores[i] > vehiclethres:\n",
    "                    box = boxes[i]\n",
    "                    matchedID = cartracker.getMatchId(img, (box[1], box[0], box[3], box[2]))\n",
    "                    if matchedID is None:\n",
    "                        carid += 1\n",
    "                        cartracker.createTrack(img, (box[1], box[0], box[3], box[2]), str(carid), scores[i], 'Bus')\n",
    "                # Class 8 represents Truck\n",
    "                elif classes[i] == 8 and scores[i] > vehiclethres:\n",
    "                    box = boxes[i]\n",
    "                    matchedID = cartracker.getMatchId(img, (box[1], box[0], box[3], box[2]))\n",
    "                    if matchedID is None:\n",
    "                        carid += 1\n",
    "                        cartracker.createTrack(img, (box[1], box[0], box[3], box[2]), str(carid), scores[i],\n",
    "                                               'Truck')\n",
    "        # detect all potential lane in the frame\n",
    "        detect_lanes(img)\n",
    "        # draw all tracked vehicle and return the count of each type\n",
    "        car, motor, bus, truck = drawTrackedVehicle(img)\n",
    "        # placing the vehicle count on top left corner\n",
    "        cv2.putText(img, 'Cars: ' + str(car), (0, 13),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0, 255, 0), 2)\n",
    "        cv2.putText(img, 'Motor: ' + str(motor), (0, 26),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255, 255, 0), 2)\n",
    "        cv2.putText(img, 'Bus: ' + str(bus), (0, 39),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (255, 0, 0), 2)\n",
    "        cv2.putText(img, 'Truck: ' + str(truck), (0, 52),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0, 255, 255), 2)\n",
    "\n",
    "        out.write(img)\n",
    "        frame_count += 1\n",
    "        # display in cv2\n",
    "        # cv2.imshow(\"preview\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError('No more frame')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
